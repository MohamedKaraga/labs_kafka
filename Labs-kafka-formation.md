# Labs Kafka - Formation Developer

## Lab 01 : Installation Sans Docker - 45min

### üéØ Objectifs
- Configurer Kafka en mode KRaft
- Cr√©er et tester des topics
- Produire et consommer des messages

### üìã Pr√©requis
- JDK 17+ install√©
- Droits administrateur

### üõ†Ô∏è Instructions

#### 1. T√©l√©charger et installer Kafka
```bash
# T√©l√©charger Kafka
wget https://downloads.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz

# Extraire
tar -xzf kafka_2.13-4.0.0.tgz
cd kafka_2.13-4.0.0

# V√©rifier Java (doit afficher version 17+)
java -version
```

#### 2. Cr√©er les r√©pertoires et configurer
```bash
# Cr√©er les dossiers de donn√©es
sudo mkdir -p /var/lib/kafka/data
sudo mkdir -p /var/lib/kafka/meta
sudo chown -R $(whoami):$(whoami) /var/lib/kafka
```

```bash
# Cr√©er la configuration dans config/server.properties
cat > config/server.properties << EOF
# Configuration KRaft
process.roles=broker,controller
node.id=1
controller.quorum.voters=1@localhost:9093

# Listeners
listeners=PLAINTEXT://localhost:9092,CONTROLLER://localhost:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT

# R√©pertoires
log.dirs=/var/lib/kafka/data
metadata.log.dir=/var/lib/kafka/meta

# Options
auto.create.topics.enable=true
EOF
```

#### 3. Formater et d√©marrer Kafka
```bash
# G√©n√©rer un UUID et formater
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/server.properties

# D√©marrer Kafka
bin/kafka-server-start.sh config/server.properties
```

#### 4. Tester dans de nouveaux terminaux
```bash
# Terminal 2 : Cr√©er un topic
bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092

# D√©crire le topic
bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092

# Terminal 3 : Producer
bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092

# Terminal 4 : Consumer
bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092
```

### ‚úÖ Validation
- [ ] Kafka d√©marre sans erreur dans le terminal 1
- [ ] Topic `test` cr√©√© avec succ√®s
- [ ] Messages tap√©s dans le producer apparaissent dans le consumer

### üîß En cas de probl√®me
```bash
# Si erreur de permissions
sudo chown -R $(whoami):$(whoami) /var/lib/kafka

# Si port occup√©, tuer les processus Kafka
ps aux | grep kafka | awk '{print $2}' | xargs kill -9
```

---

## Lab 02 : Kafka avec Docker - 30min

### üéØ Objectifs
- D√©ployer Kafka avec Docker
- Utiliser Control Center pour le monitoring
- Comprendre l'architecture conteneuris√©e

### üìã Pr√©requis
- Docker et Docker Compose install√©s
- 8GB RAM disponible

### üõ†Ô∏è Instructions

#### 1. R√©cup√©rer le fichier Docker Compose
```bash
# T√©l√©charger la configuration
wget https://raw.githubusercontent.com/MohamedKaraga/labs_kafka/refs/heads/master/docker-compose.yml

# V√©rifier que Docker fonctionne
docker ps
```

#### 2. D√©marrer les services
```bash
# D√©marrer Kafka et Control Center
docker-compose up -d broker control-center

# V√©rifier le d√©marrage (attendre ~30 secondes)
docker-compose ps
```

#### 3. Tester Kafka
```bash
# Entrer dans le conteneur broker
docker-compose exec broker /bin/bash

# Dans le conteneur : cr√©er un topic
kafka-topics --bootstrap-server broker:9092 --create --topic test --partitions 1 --replication-factor 1

# Lister les topics
kafka-topics --bootstrap-server broker:9092 --list
```

#### 4. Test Producer/Consumer
```bash
# Terminal 1 : Producer (dans le conteneur)
kafka-console-producer --bootstrap-server broker:9092 --topic test

# Terminal 2 : Consumer (nouveau terminal, entrer dans le conteneur)
docker-compose exec broker /bin/bash
kafka-console-consumer --bootstrap-server broker:9092 --from-beginning --topic test
```

### ‚úÖ Validation
- [ ] Conteneurs `broker` et `control-center` en statut "Up"
- [ ] Control Center accessible sur http://localhost:9021
- [ ] Messages √©chang√©s entre producer et consumer

### üîß En cas de probl√®me
```bash
# Red√©marrer proprement
docker-compose down -v
docker-compose up -d broker control-center

# Voir les logs en cas d'erreur
docker-compose logs broker
```

## Lab 03 : Producer {collapsible="true"}

* Simuler des producteurs Kafka int√©gr√© dans des appareils IoT servant √† collecter les donn√©es des capteurs.
* Configurer le producteur pour publier les donn√©es des capteurs dans un topic Kafka.
* Configurer le batching, la compression pour optimiser la transmission des donn√©es.

1. Ex√©cutez la commande suivante dans le terminal pour cloner le projet :

   ```bash
   git clone https://github.com/MohamedKaraga/labs_kafka.git
   ```

2. Allez dans le module producteur
   ```bash
   cd labs_kafka/producer
   ```
3. completez le code pour `ProducerConfig`, `ProducerRecord`, and `KafkaProducer`.
4. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:
   ```bash
   docker-compose up -d broker control-center
   ```
5. Construire et ex√©cuter le producteur
6. Ex√©cutez le producteur avec diff√©rentes configurations pour `linger.ms` et `batch.size` afin d'observer leurs effets sur le batching des messages et le temps d'envoi.
7. Configurer le type de compression pour optimiser la transmission des donn√©es(`snappy` ou `gzip` ou `lz4`)
8. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```
   
## Lab 04 : Consumer {collapsible="true"}

* Configurer le consommateur Kafka pour ing√©rer les donn√©es des capteurs depuis un topic Kafka en temps r√©el.
* Traiter les donn√©es entrantes
* Vous apprendrez comment ajuster des param√®tres tels que `fetch.min.bytes`, `fetch.max.wait.ms`, `max.poll.records` pour obtenir des performances optimales.

1. Ex√©cutez la commande suivante dans le terminal pour cloner le projet (cette √©tape n'est pas n√©cessaire si vous l'avez d√©j√† faite dans le lab pr√©c√©dent):

   ```bash
   git clone https://github.com/MohamedKaraga/labs_kafka.git
   ```

2. Allez dans le module consommateur
   ```bash
   cd labs_kafka/consumer
   ```
3. completez le code pour `ConsumerConfig`, `KafkaConsumer`.
4. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:
   ```bash
   docker-compose up -d broker control-center
   ```
5. Construire et ex√©cuter le consommateur
6. Relancez le producer utilis√© dans le `Lab 03` pour alimenter le topic 

7. Ex√©cutez le consommateur avec les differentes configurations pour `fetch.min.bytes (5000000)`, `fetch.max.wait.ms (5000)` pour observer leurs effets sur la consommation des messages et les performances

8. Conversion d'un Consommateur Kafka avec Commit Automatique vers Commit Manuel

Vous disposez actuellement d'un consommateur Kafka qui utilise le m√©canisme de commit automatique des offsets. Cependant, votre √©quipe a identifi√© des probl√®mes potentiels de fiabilit√© dans le pipeline de donn√©es lors d'incidents syst√®me. Pour am√©liorer la robustesse de votre application, vous devez impl√©menter un m√©canisme de commit manuel.

Les objectifs sont : 

- Comprendre les diff√©rences entre commit automatique et manuel dans Kafka
- Impl√©menter une strat√©gie de commit manuel qui garantit le traitement "exactement une fois" (exactly-once)
- Tester la r√©sistance de votre solution aux pannes

9. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```

## Lab 05 : Kafka REST Proxy {collapsible="true"}

1. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:
   ```bash
   docker-compose up -d broker control-center rest-proxy
   ```
2. Produire des messages dans un topic Kafka en utilisant le REST Proxy

   * Ex√©cuter dans la console du conteneur `broker`
      ```bash
      docker-compose exec broker /bin/bash
      ```
   * Cr√©er un topic kafka
   ```bash
   kafka-topics --create --topic bar --bootstrap-server broker:9092 --partitions 1 --replication-factor 1
   ```
   * Produire un message JSON dans le topic
   ```bash
   curl -X POST -H "Content-Type: application/vnd.kafka.json.v2+json" \
   --data '{"records":[{"value":{"name":"toto", "age":30}}]}' \
   http://localhost:8082/topics/bar
   ```
3. Consommer des messages dans un topic Kafka en utilisant le REST Proxy
   * Cr√©er une nouvelle instance consumer
   ```bash
   curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
   --data '{"name": "my_consumer_instance", "format": "json", "auto.offset.reset": "earliest"}' \
   http://localhost:8082/consumers/my_consumer_group
   ```   
   * Abonner le consommateur au topic
   ```bash
   curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
   --data '{"topics":["bar"]}' \
   http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/subscription
   ```   
   * Consommer des messages
   ```bash
   curl -X GET -H "Accept: application/vnd.kafka.json.v2+json" \
   http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/records
   ```
5. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```

## Lab 06 : Schema Registry {collapsible="true"}

1. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:

   ```bash
   docker-compose up -d broker control-center schema-registry
   ```
2. Enregistrer le sch√©ma

   ```bash
   curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
   --data '{"schema": "{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"},{\"name\":\"email\",\"type\":[\"null\",\"string\"],\"default\":null}]}"}' \
   http://localhost:8081/subjects/user-value/versions
   ```
3. Produire et consommer des messages en utilisant le sch√©ma avro

   * Transformez votre producer **Lab 03** et consumer **Lab 04** en utilisant le sch√©ma et l'API schema registry.
   * Ajouter le repository maven de confluent
   ```xml
    <repositories>
    <repository>
      <id>confluent</id>
      <url>https://packages.confluent.io/maven/</url>
    </repository>
   </repositories>     
   ```
   * Ajouter les dependances Maven Avro et Schema Registry

        ```xml
       <!-- Avro dependencies -->
       <dependency>
           <groupId>org.apache.avro</groupId>
           <artifactId>avro</artifactId>
           <version>1.11.1</version>
       </dependency>
       <!-- Schema Registry dependencies -->
       <dependency>
          <groupId>io.confluent</groupId>
          <artifactId>kafka-avro-serializer</artifactId>
          <version>7.6.0</version>
       </dependency>
      ```

   * Cr√©er le repertoire `src/main/avro` et definir le sch√©ma Avro (user.avsc)

     ```json
        {
       "type": "record",
       "name": "User",
       "namespace": "com.example.avro",
       "fields": [
         {"name": "name", "type": "string"},
         {"name": "age", "type": "int"}
       ]
       }
     ```

   * Ajoutez le plugin Avro Maven √† votre `pom.xml` du producteur et du consommateur pour g√©n√©rer des classes Java √† partir du sch√©ma Avro

     ```xml
     <build>
      <plugins>
          <plugin>
              <groupId>org.apache.avro</groupId>
              <artifactId>avro-maven-plugin</artifactId>
              <version>1.11.1</version>
              <executions>
                  <execution>
                      <phase>generate-sources</phase>
                      <goals>
                          <goal>schema</goal>
                      </goals>
                      <configuration>
                          <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>
                          <outputDirectory>${project.basedir}/src/main/java</outputDirectory>
                      </configuration>
                  </execution>
              </executions>
          </plugin>
      </plugins>
     </build>
     ```
   
   * Ex√©cutez la commande Maven pour g√©n√©rer les classes Java

        ```bash
        mvn clean compile
        ```
  
   * Compl√©tez et ex√©cutez pour produire avec le sch√©ma Avro
   
     ```Java
          Properties props = new Properties();
          props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
          props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
          props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
          props.put("schema.registry.url", "http://localhost:8081");

          KafkaProducer<String, User> producer = new KafkaProducer<>(props);

          User user = new User("John Doe", 30);

          ProducerRecord<String, User> record = new ProducerRecord<>("users", user.getName().toString(), user);
     ```
     
   * Ex√©cutez la classe Producer et assurez-vous qu'un message est produit dans le topic `users`
  
4. Consommer des messages en utilisant le sch√©ma avro

   * Transformez votre consommateur `Lab 04` en utilisant le sch√©ma et l'API du Schema Registry.
   * Ajoutez les d√©pendances Maven Avro et Schema Registry comme pr√©c√©demment.
   * Ajoutez le plugin Avro Maven √† votre `pom.xml` pour g√©n√©rer des classes Java √† partir du sch√©ma Avro comme pr√©c√©demment.

     ```Java
           Properties props = new Properties();
           props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
           props.put(ConsumerConfig.GROUP_ID_CONFIG, "user-consumer-group");
           props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
           props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
           props.put("schema.registry.url", "http://localhost:8081");
           props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

           KafkaConsumer<String, User> consumer = new KafkaConsumer<>(props);
     ```
     
   * Ex√©cutez la classe Consumer et v√©rifiez que le consommateur lit le message du topic `users`.
     
5. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```


## Lab 07 : Deployer un kafka connect {collapsible="true"}


1. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:

   ```bash
   docker-compose up -d broker control-center postgres connect mongodb
   ```
   
2. Installer un connecteur JDBC

   ```bash
   docker-compose exec -u root connect confluent-hub install confluentinc/kafka-connect-jdbc:10.7.6
   ```
   
   **Output**
   ```Console
   The component can be installed in any of the following Confluent Platform installations: 
   1. / (installed rpm/deb package)
   2. / (where this tool is installed)
      Choose one of these to continue the installation (1-2): 1
      Do you want to install this into /usr/share/confluent-hub-components? (yN) N
   
   Specify installation directory: /usr/share/java/kafka
   
   Component's license:
   Confluent Community License
   https://www.confluent.io/confluent-community-license
   I agree to the software license agreement (yN) y
   
   Downloading component Kafka Connect JDBC 10.7.6, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/java/kafka
   Detected Worker's configs:
   1. Standard: /etc/kafka/connect-distributed.properties
   2. Standard: /etc/kafka/connect-standalone.properties
   3. Standard: /etc/schema-registry/connect-avro-distributed.properties
   4. Standard: /etc/schema-registry/connect-avro-standalone.properties
   5. Used by Connect process with PID : /etc/kafka-connect/kafka-connect.properties
      Do you want to update all detected configs? (yN) y
   
   Adding installation directory to plugin path in the following files:
   /etc/kafka/connect-distributed.properties
   /etc/kafka/connect-standalone.properties
   /etc/schema-registry/connect-avro-distributed.properties
   /etc/schema-registry/connect-avro-standalone.properties
   /etc/kafka-connect/kafka-connect.properties
   
   Completed

   ```
   
3. Installer un connecteur MongoDB

   ```bash
   docker-compose exec -u root connect confluent-hub install mongodb/kafka-connect-mongodb:latest
   ```
   
   **Output**
   ```Console
   The component can be installed in any of the following Confluent Platform installations: 
   1. / (installed rpm/deb package)
   2. / (where this tool is installed)
      Choose one of these to continue the installation (1-2): 1
      Do you want to install this into /usr/share/confluent-hub-components? (yN) N
   
   Specify installation directory: /usr/share/java/kafka
   
   Component's license:
   Confluent Community License
   https://www.confluent.io/confluent-community-license
   I agree to the software license agreement (yN) y
   
   Downloading component Kafka Connect JDBC 10.7.6, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/java/kafka
   Detected Worker's configs:
   1. Standard: /etc/kafka/connect-distributed.properties
   2. Standard: /etc/kafka/connect-standalone.properties
   3. Standard: /etc/schema-registry/connect-avro-distributed.properties
   4. Standard: /etc/schema-registry/connect-avro-standalone.properties
   5. Used by Connect process with PID : /etc/kafka-connect/kafka-connect.properties
      Do you want to update all detected configs? (yN) y
   
   Adding installation directory to plugin path in the following files:
   /etc/kafka/connect-distributed.properties
   /etc/kafka/connect-standalone.properties
   /etc/schema-registry/connect-avro-distributed.properties
   /etc/schema-registry/connect-avro-standalone.properties
   /etc/kafka-connect/kafka-connect.properties
   
   Completed

   ```
4. Redemarrer le connecteur

   ```bash
   docker-compose restart connect
   ```

5. A l'interieur du conteneur PostgreSQL, cr√©er une table `users`

   ```bash
   docker-compose exec postgres psql -U myuser -d lab
   ```

   ```SQL
   CREATE TABLE users (
   id SERIAL PRIMARY KEY,
   name VARCHAR(100),
   email VARCHAR(100),
   created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
   );
   ```
6. Cr√©ez le fichier `jdbc-source-config.json` pour le connecteur JDBC Source afin d'extraire les donn√©es de cette table users et de les publier dans un topic Kafka.

   ```JSON
   {
   "name":"jdbc-source-connector",
   "config":{
   "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
   "tasks.max":"1",
   "connection.url":"jdbc:postgresql://postgres:5432/lab",
   "connection.user":"myuser",
   "connection.password":"mypassword",
   "table.whitelist":"users",
   "mode":"incrementing",
   "incrementing.column.name":"id",
   "poll.interval.ms":"10000",
   "key.converter": "org.apache.kafka.connect.json.JsonConverter",
   "value.converter": "org.apache.kafka.connect.json.JsonConverter",
   "key.converter.schemas.enable": "false",
   "value.converter.schemas.enable": "false"
   }
   }
   ```
   
7. Deployer le connecteur Source JDBC

   ```bash
   curl -X POST -H "Content-Type: application/json" --data @jdbc-source-config.json http://connect:8083/connectors
   ```
8. V√©rifiez l'√©tat du connecteur JDBC Source

   ```bash
   curl -X GET http://connect:8083/connectors/jdbc-source-connector/status
   ```

9. Cr√©ez le fichier `mongo-sink-config.json` pour le connecteur MongoDB Sink afin d'extraire les donn√©es de ce topic users et de les stocker dans MongoDB.

   ```JSON
   {
   "name": "mongodb-sink-connector",
   "config": {
   "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
   "tasks.max": "1",
   "topics": "users",
   "connection.uri": "mongodb://myuser:mypassword@mongodb:27017",
   "database": "lab",
   "collection": "users",
   "key.converter": "org.apache.kafka.connect.json.JsonConverter",
   "key.converter.schemas.enable": "false",
   "value.converter": "org.apache.kafka.connect.json.JsonConverter",
   "value.converter.schemas.enable": "false"
    }
   }
   ```

10. Deployer le connecteur MongoDB Sink 

   ```bash
   curl -X POST -H "Content-Type: application/json" --data @mongo-sink-config.json http://connect:8083/connectors
   ```
11. V√©rifiez l'√©tat du connecteur Mongo Sink

   ```bash
   curl -X GET http://connect:8083/connectors/mongodb-sink-connector/status
   ```

12. Acc√©dez au conteneur PostgreSQL et ins√©rez des donn√©es d'exemple dans la table `users`

   ```bash
   docker-compose exec postgres psql -U myuser -d lab
   ```

   ```SQL
   INSERT INTO users (name, email) VALUES
   ('toto', 'toto@example.com'),
   ('titi', 'titi@example.com'),
   ('tata', 'tata@example.com');
   ```
13. V√©rifiez les donn√©es dans MongoDB

    * A l'int√©rieur du conteneur
       ```bash
       docker-compose exec mongodb /bin/bash
       ```
    * Se connecter au serveur

        ```bash
        mongosh "mongodb://myuser:mypassword@mongodb"
        ```
    * utiliser la base de donn√©es `lab`
        ```bash
        use lab;
        ```
    * faire une requete `find`
        ```bash
        db.users.find();
        ```
14. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```
   
## Lab 08 : Basic kafka stream {collapsible="true"}

* Vous allez cr√©er une application Kafka Streams simple qui lit des donn√©es depuis un topic source
* Traiter les donn√©es
* Puis √©crire les r√©sultats dans un topic de destination

1. Ex√©cutez la commande suivante dans le terminal pour cloner le projet (cette √©tape n'est pas n√©cessaire si vous l'avez d√©j√† faite dans le laboratoire pr√©c√©dent)

   ```bash
   git clone https://github.com/MohamedKaraga/labs_kafka.git
   ```
2. Allez dans le module kafkastream
   ```bash
   cd labs_kafka/kafkastream
   ```
3. completez le code pour `StreamsConfig`, `StreamsBuilder` et `KafkaStreams`.
4. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:
   ```bash
   docker-compose up -d broker control-center
   ```
5. Construire et ex√©cuter kafkastream

6. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```

## Lab 09 : KsqlDB {collapsible="true"}

1. D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©:

   ```bash
   docker-compose up -d broker control-center ksqldb-server ksqldb-cli
   ```
   
2. Utilisez la console Kafka pour produire des messages vers le topic `users`

   * Ex√©cuter dans la console du conteneur `broker`
      ```bash
      docker-compose exec broker /bin/bash
      ```

   * Ex√©cuter le producteur

      ```bash
      kafka-console-producer --bootstrap-server kafka:9092 --topic users
      ``` 

3. Saisissez les messages JSON suivants, chacun repr√©sentant un enregistrement `user`

   ```JSON
   {"id": 1, "name": "toto", "email": "toto.doe@example.com", "created_at": "2024-06-23T12:00:00Z"}
   ```   

   ```JSON
   {"id": 2, "name": "titi", "email": "titi@example.com", "created_at": "2024-05-23T12:05:00Z"}
   ```

   ```JSON
   {"id": 3, "name": "tata", "email": "tata@example.com", "created_at": "2024-06-23T12:05:00Z"}
   ```

   ```JSON
   {"id": 4, "name": "jo", "email": "jo.doe@example.com", "created_at": "2024-05-23T12:00:00Z"}
   ```   

   ```JSON
   {"id": 5, "name": "ohe", "email": "ohe@example.com", "created_at": "2024-05-23T12:05:00Z"}
   ```

   ```JSON
   {"id": 6, "name": "yao", "email": "yao@example.com", "created_at": "2024-06-23T12:05:00Z"}
   ```
   
4. Quittez le conteneur `broker` et acc√©dez √† l'interface web du serveur ksqlDB en naviguant vers `http://localhost:8088` dans votre navigateur. Cela devrait afficher l'interface du serveur ksqlDB.

5. Acc√©dez √† l'interface en ligne de commande (CLI) de ksqlDB pour ex√©cuter des requ√™tes ksqlDB

   ```bash
   docker-compose exec ksqldb-cli bash
   ```
6. A l'int√©rieur du shell, connectez-vous au serveur ksqlDB en utilisant l'interface en ligne de commande (CLI) de ksqlDB

   * Connectez-vous au serveur ksqlDB
   ```bash
   ksql http://ksqldb-server:8088
   ```
   * D√©finir l'offset sur Earliest
   ```bash
   SET 'auto.offset.reset' = 'earliest';
   ```

7. Cr√©ez un stream dans ksqlDB en utilisant la requ√™te suivante. Par exemple, pour cr√©er un stream pour le topic `users`

   ```SQL
   CREATE STREAM users_stream (id INT, name VARCHAR, email VARCHAR, created_at VARCHAR) WITH (KAFKA_TOPIC='users',VALUE_FORMAT='JSON');
   ```
8. Interrogez le `users_stream` pour voir les donn√©es

   ```SQL
   SELECT * FROM users_stream EMIT CHANGES;
   ```
9. Cr√©e une table appel√©e `user_counts` qui stocke le nombre `users` group√©s par leur `created_at`

   ```SQL
   CREATE TABLE user_counts AS
   SELECT created_at, COUNT(*) AS count
   FROM users_stream
   GROUP BY created_at;
   ```
10. Interrogez `user_counts` pour voir les donn√©es

   ```SQL
   SELECT * FROM user_counts EMIT CHANGES;
   ```

11. Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes

   ```bash
   docker-compose down -v
   ```

   
Vous pouvez m'envoyer un e-mail √† [mohamedkaraga@yahoo.fr](mailto:mohamedkaraga@yahoo.fr).
