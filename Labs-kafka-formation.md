# Labs Kafka - Formation Developer

## Lab 01 : Installation Sans Docker - 60min

### üéØ Objectifs
- Configurer Kafka en mode KRaft
- Cr√©er et tester des topics
- Produire et consommer des messages

### üìã Pr√©requis
- **JDK 11+ install√©** (JDK 17+ recommand√© pour Kafka 4.0)
- Droits administrateur

**üí° Choix de version :**
- **Kafka 3.9** : Compatible avec Java 11+ (recommand√© pour la compatibilit√©)
- **Kafka 4.0** : N√©cessite obligatoirement Java 17+

### üõ†Ô∏è Instructions

#### 1. T√©l√©charger et installer Kafka
```bash
# Option A : Kafka 3.9 (compatible Java 11+) - RECOMMAND√â
wget https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz
tar -xzf kafka_2.13-3.9.0.tgz
cd kafka_2.13-3.9.0

# Option B : Kafka 4.0 (n√©cessite Java 17+)
# wget https://downloads.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
# tar -xzf kafka_2.13-4.0.0.tgz
# cd kafka_2.13-4.0.0

# V√©rifier Java (doit afficher version 11+ pour Kafka 3.9 ou 17+ pour Kafka 4.0)
java -version
```

#### 2. Cr√©er les r√©pertoires et configurer
```bash
# Cr√©er les dossiers de donn√©es
sudo mkdir -p /var/lib/kafka/data
sudo mkdir -p /var/lib/kafka/meta
sudo chown -R $(whoami):$(whoami) /var/lib/kafka
```

```bash
# Cr√©er la configuration dans config/server.properties
cat > config/server.properties << EOF
# Configuration KRaft
process.roles=broker,controller
node.id=1
controller.quorum.voters=1@localhost:9093

# Listeners
listeners=PLAINTEXT://localhost:9092,CONTROLLER://localhost:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT

# R√©pertoires
log.dirs=/var/lib/kafka/data
metadata.log.dir=/var/lib/kafka/meta

# Options
auto.create.topics.enable=true
EOF
```

#### 3. Formater et d√©marrer Kafka
```bash
# G√©n√©rer un UUID et formater
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/server.properties

# D√©marrer Kafka
bin/kafka-server-start.sh config/server.properties
```

#### 4. Tester dans de nouveaux terminaux
```bash
# Terminal 2 : Cr√©er un topic
bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092

# D√©crire le topic
bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092

# Terminal 3 : Producer
bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092

# Terminal 4 : Consumer
bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092
```

**Exemples de messages √† saisir dans le producer (Terminal 3) :**
```
Formation Kafka avec Orsys
Hello World Kafka
{"event": "user_login", "user_id": 123, "timestamp": "2024-12-16T10:30:00Z"}
{"sensor_id": "TEMP_001", "temperature": 22.5, "location": "Paris"}
```

_Les messages appara√Ætront imm√©diatement dans le consumer (Terminal 4)_

### ‚úÖ Validation
- [ ] Kafka d√©marre sans erreur dans le terminal 1
- [ ] Topic `test` cr√©√© avec succ√®s
- [ ] Messages tap√©s dans le producer apparaissent dans le consumer

### üîß En cas de probl√®me
```bash
# Si erreur "UnsupportedClassVersionError" ou "class file version 61.0"
# Cela signifie incompatibilit√© Java/Kafka
java -version

# Solutions selon la version Kafka choisie :
# Pour Kafka 3.9 : Java 11+ suffit
# Ubuntu/Debian: sudo apt update && sudo apt install openjdk-11-jdk
# CentOS/RHEL: sudo yum install java-11-openjdk-devel
# macOS: brew install openjdk@11

# Pour Kafka 4.0 : Java 17+ obligatoire
# Ubuntu/Debian: sudo apt update && sudo apt install openjdk-17-jdk
# CentOS/RHEL: sudo yum install java-17-openjdk-devel
# macOS: brew install openjdk@17

# Puis d√©finir JAVA_HOME si n√©cessaire
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
# ou sur macOS: export JAVA_HOME=/opt/homebrew/opt/openjdk@11

# Si erreur de permissions
sudo chown -R $(whoami):$(whoami) /var/lib/kafka

# Si port occup√©, tuer les processus Kafka
ps aux | grep kafka | awk '{print $2}' | xargs kill -9

# Si erreur "Invalid cluster.id" :
# 1. Arr√™ter Kafka
ps aux | grep kafka | awk '{print $2}' | xargs kill -9

# 2. Nettoyer compl√®tement les r√©pertoires de donn√©es
sudo rm -rf /var/lib/kafka/data/*
sudo rm -rf /var/lib/kafka/meta/*

# 3. Reformater avec un nouvel UUID
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/server.properties

# 4. Red√©marrer Kafka
bin/kafka-server-start.sh config/server.properties
```

---

## Lab 02 : Kafka avec Docker - 45min

### üéØ Objectifs
- D√©ployer Kafka avec Docker
- Utiliser Control Center pour le monitoring
- Comprendre l'architecture conteneuris√©e

### üìã Pr√©requis
- Docker et Docker Compose install√©s
- 8GB RAM disponible

### üõ†Ô∏è Instructions

#### 1. R√©cup√©rer le fichier Docker Compose
```bash
# T√©l√©charger la configuration
wget https://raw.githubusercontent.com/MohamedKaraga/labs_kafka/refs/heads/master/docker-compose.yml

# V√©rifier que Docker fonctionne
docker ps
```

#### 2. D√©marrer les services
```bash
# D√©marrer Kafka et Control Center
docker-compose up -d broker control-center

# V√©rifier le d√©marrage (attendre ~30 secondes)
docker-compose ps
```

#### 3. Tester Kafka
```bash
# Entrer dans le conteneur broker
docker-compose exec broker /bin/bash

# Dans le conteneur : cr√©er un topic
kafka-topics --bootstrap-server broker:9092 --create --topic test --partitions 1 --replication-factor 1

# Lister les topics
kafka-topics --bootstrap-server broker:9092 --list
```

#### 4. Test Producer/Consumer
```bash
# Terminal 1 : Producer (dans le conteneur)
kafka-console-producer --bootstrap-server broker:9092 --topic test

# Terminal 2 : Consumer (nouveau terminal, entrer dans le conteneur)
docker-compose exec broker /bin/bash
kafka-console-consumer --bootstrap-server broker:9092 --from-beginning --topic test
```

### ‚úÖ Validation
- [ ] Conteneurs `broker` et `control-center` en statut "Up"
- [ ] Control Center accessible sur http://localhost:9021
- [ ] Messages √©chang√©s entre producer et consumer

### üîß En cas de probl√®me
```bash
# Red√©marrer proprement
docker-compose down -v
docker-compose up -d broker control-center

# Voir les logs en cas d'erreur
docker-compose logs broker
```

---

## Lab 03 : Producer - 60min

### üéØ Objectifs
- Simuler des producteurs Kafka int√©gr√©s dans des appareils IoT pour collecter les donn√©es des capteurs
- Configurer le producteur pour publier les donn√©es des capteurs dans un topic Kafka
- Configurer le batching et la compression pour optimiser la transmission des donn√©es

### üìã Pr√©requis
- Java et Maven install√©s
- Cluster Kafka fonctionnel (Lab 02)
- Notions de programmation Java

### üõ†Ô∏è Instructions

#### 1. Cloner le projet
```bash
# Ex√©cuter la commande suivante dans le terminal pour cloner le projet
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module producteur
```bash
cd labs_kafka/producer
```

#### 3. Compl√©ter le code
Completez le code pour `ProducerConfig`, `ProducerRecord`, et `KafkaProducer`.

#### 4. D√©marrer le cluster Kafka
```bash
# D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter le producteur
Construire et ex√©cuter le producteur selon vos pr√©f√©rences (IDE ou ligne de commande).

#### 6. Tester les configurations de performance
Ex√©cutez le producteur avec diff√©rentes configurations pour `linger.ms` et `batch.size` afin d'observer leurs effets sur le batching des messages et le temps d'envoi.

#### 7. Configurer la compression
Configurez le type de compression pour optimiser la transmission des donn√©es (`snappy` ou `gzip` ou `lz4`).

### ‚úÖ Validation
- [ ] Code compile et s'ex√©cute sans erreur
- [ ] Messages des capteurs visibles dans Control Center
- [ ] Diff√©rences de performance observ√©es avec diff√©rentes configurations
- [ ] Impact de la compression analys√©

### üîß En cas de probl√®me
```bash
# V√©rifier l'√©tat des conteneurs
docker-compose ps

# Voir les logs
docker-compose logs broker

# Red√©marrer l'environnement
docker-compose down -v
docker-compose up -d broker control-center
```

#### 8. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 04 : Consumer - 60min

### üéØ Objectifs
- Configurer le consommateur Kafka pour ing√©rer les donn√©es des capteurs depuis un topic Kafka en temps r√©el
- Traiter les donn√©es entrantes
- Apprendre √† ajuster des param√®tres tels que `fetch.min.bytes`, `fetch.max.wait.ms`, `max.poll.records` pour obtenir des performances optimales
- Impl√©menter la conversion du commit automatique vers commit manuel

### üìã Pr√©requis
- Producer du Lab 03 fonctionnel
- Java et Maven install√©s
- Compr√©hension des concepts de consumer groups

### üõ†Ô∏è Instructions

#### 1. Cloner le projet (si pas d√©j√† fait)
```bash
# Ex√©cuter la commande suivante dans le terminal pour cloner le projet (cette √©tape n'est pas n√©cessaire si vous l'avez d√©j√† faite dans le lab pr√©c√©dent)
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module consommateur
```bash
cd labs_kafka/consumer
```

#### 3. Compl√©ter le code
Completez le code pour `ConsumerConfig` et `KafkaConsumer`.

#### 4. D√©marrer le cluster Kafka
```bash
# D√©marrez le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter le consommateur
Construire et ex√©cuter le consommateur.

#### 6. Alimenter le topic
Relancez le producer utilis√© dans le `Lab 03` pour alimenter le topic.

#### 7. Tester les configurations de performance
Ex√©cutez le consommateur avec les diff√©rentes configurations pour `fetch.min.bytes (5000000)`, `fetch.max.wait.ms (5000)` pour observer leurs effets sur la consommation des messages et les performances.

#### 8. Conversion vers commit manuel
**Conversion d'un Consommateur Kafka avec Commit Automatique vers Commit Manuel**

Vous disposez actuellement d'un consommateur Kafka qui utilise le m√©canisme de commit automatique des offsets. Cependant, votre √©quipe a identifi√© des probl√®mes potentiels de fiabilit√© dans le pipeline de donn√©es lors d'incidents syst√®me. Pour am√©liorer la robustesse de votre application, vous devez impl√©menter un m√©canisme de commit manuel.

**Les objectifs sont :**
- Comprendre les diff√©rences entre commit automatique et manuel dans Kafka
- Impl√©menter une strat√©gie de commit manuel qui garantit le traitement "exactement une fois" (exactly-once)
- Tester la r√©sistance de votre solution aux pannes

### ‚úÖ Validation
- [ ] Consumer re√ßoit et traite les messages
- [ ] Configurations de performance test√©es et analys√©es
- [ ] Commit automatique puis manuel impl√©ment√©s
- [ ] Robustesse test√©e (r√©sistance aux pannes)
- [ ] M√©triques visibles dans Control Center

### üîß En cas de probl√®me
```bash
# V√©rifier les consumer groups
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --list

# Voir les d√©tails du groupe
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --describe --group [group-name]

# Reset des offsets si n√©cessaire
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --group [group-name] --reset-offsets --to-earliest --topic [topic-name] --execute
```

#### 9. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 05 : Kafka REST Proxy - 45min

### üéØ Objectifs
- Utiliser l'API REST de Kafka pour produire et consommer des messages
- Comprendre l'int√©gration HTTP avec Kafka
- Manipuler des donn√©es JSON via REST

### üìã Pr√©requis
- Docker et Docker Compose install√©s
- Notions de base des API REST et JSON
- Compr√©hension des concepts curl

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec REST Proxy
```bash
# D√©marrer le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center rest-proxy
```

#### 2. Produire des messages dans un topic Kafka en utilisant le REST Proxy
```bash
# Ex√©cuter dans la console du conteneur `broker`
docker-compose exec broker /bin/bash
```

```bash
# Cr√©er un topic kafka
kafka-topics --create --topic bar --bootstrap-server broker:9092 --partitions 1 --replication-factor 1
```

```bash
# Produire un message JSON dans le topic
curl -X POST -H "Content-Type: application/vnd.kafka.json.v2+json" \
--data '{"records":[{"value":{"name":"toto", "age":30}}]}' \
http://localhost:8082/topics/bar
```

#### 3. Consommer des messages dans un topic Kafka en utilisant le REST Proxy
```bash
# Cr√©er une nouvelle instance consumer
curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
--data '{"name": "my_consumer_instance", "format": "json", "auto.offset.reset": "earliest"}' \
http://localhost:8082/consumers/my_consumer_group
```

```bash
# Abonner le consommateur au topic
curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
--data '{"topics":["bar"]}' \
http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/subscription
```

```bash
# Consommer des messages
curl -X GET -H "Accept: application/vnd.kafka.json.v2+json" \
http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/records
```

### ‚úÖ Validation
- [ ] REST Proxy accessible sur le port 8082
- [ ] Messages JSON produits avec succ√®s
- [ ] Messages consomm√©s via l'API REST
- [ ] Consumer instance cr√©√©e et utilis√©e correctement

### üîß En cas de probl√®me
```bash
# V√©rifier l'√©tat du REST Proxy
docker-compose logs rest-proxy

# Tester la connectivit√©
curl http://localhost:8082/topics

# Red√©marrer les services
docker-compose down -v
docker-compose up -d broker control-center rest-proxy
```

#### 4. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 06 : Schema Registry - 60min

### üéØ Objectifs
- Comprendre la gestion des sch√©mas avec Schema Registry
- Utiliser Apache Avro pour la s√©rialisation
- Transformer les producers/consumers existants pour utiliser des sch√©mas
- G√©n√©rer des classes Java √† partir de sch√©mas Avro

### üìã Pr√©requis
- Java et Maven install√©s
- Producer et Consumer des labs pr√©c√©dents
- Connaissance des concepts de s√©rialisation

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec Schema Registry
```bash
# D√©marrer le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center schema-registry
```

#### 2. Enregistrer le sch√©ma
```bash
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"},{\"name\":\"email\",\"type\":[\"null\",\"string\"],\"default\":null}]}"}' \
http://localhost:8081/subjects/user-value/versions
```

#### 3. Produire et consommer des messages en utilisant le sch√©ma avro

**Transformer votre producer Lab 03 et consumer Lab 04** en utilisant le sch√©ma et l'API schema registry.

**Ajouter le repository maven de confluent :**
```xml
<repositories>
  <repository>
    <id>confluent</id>
    <url>https://packages.confluent.io/maven/</url>
  </repository>
</repositories>
```

**Ajouter les d√©pendances Maven Avro et Schema Registry :**
```xml
<!-- Avro dependencies -->
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.1</version>
</dependency>
<!-- Schema Registry dependencies -->
<dependency>
   <groupId>io.confluent</groupId>
   <artifactId>kafka-avro-serializer</artifactId>
   <version>7.6.0</version>
</dependency>
```

**Cr√©er le r√©pertoire `src/main/avro` et d√©finir le sch√©ma Avro (user.avsc) :**
```json
{
  "type": "record",
  "name": "User",
  "namespace": "com.example.avro",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"}
  ]
}
```

**Ajouter le plugin Avro Maven** √† votre `pom.xml` du producteur et du consommateur pour g√©n√©rer des classes Java √† partir du sch√©ma Avro :
```xml
<build>
  <plugins>
      <plugin>
          <groupId>org.apache.avro</groupId>
          <artifactId>avro-maven-plugin</artifactId>
          <version>1.11.1</version>
          <executions>
              <execution>
                  <phase>generate-sources</phase>
                  <goals>
                      <goal>schema</goal>
                  </goals>
                  <configuration>
                      <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>
                      <outputDirectory>${project.basedir}/src/main/java</outputDirectory>
                  </configuration>
              </execution>
          </executions>
      </plugin>
  </plugins>
</build>
```

#### 4. G√©n√©rer les classes Java
```bash
# Ex√©cuter la commande Maven pour g√©n√©rer les classes Java
mvn clean compile
```

#### 5. Compl√©ter le Producer avec Avro
**Compl√©ter et ex√©cuter pour produire avec le sch√©ma Avro :**
```java
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
props.put("schema.registry.url", "http://localhost:8081");

KafkaProducer<String, User> producer = new KafkaProducer<>(props);

User user = new User("John Doe", 30);

ProducerRecord<String, User> record = new ProducerRecord<>("users", user.getName().toString(), user);
```

**Ex√©cuter la classe Producer** et s'assurer qu'un message est produit dans le topic `users`.

#### 6. Consommer des messages en utilisant le sch√©ma avro

**Transformer votre consommateur Lab 04** en utilisant le sch√©ma et l'API du Schema Registry.

Ajouter les d√©pendances Maven Avro et Schema Registry comme pr√©c√©demment.

Ajouter le plugin Avro Maven √† votre `pom.xml` pour g√©n√©rer des classes Java √† partir du sch√©ma Avro comme pr√©c√©demment.

```java
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.GROUP_ID_CONFIG, "user-consumer-group");
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
props.put("schema.registry.url", "http://localhost:8081");
props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

KafkaConsumer<String, User> consumer = new KafkaConsumer<>(props);
```

**Ex√©cuter la classe Consumer** et v√©rifier que le consommateur lit le message du topic `users`.

### ‚úÖ Validation
- [ ] Schema Registry accessible sur le port 8081
- [ ] Sch√©ma User enregistr√© correctement
- [ ] Classes Java g√©n√©r√©es √† partir du sch√©ma
- [ ] Producer envoie des messages Avro
- [ ] Consumer re√ßoit et d√©s√©rialise les messages Avro

### üîß En cas de probl√®me
```bash
# V√©rifier Schema Registry
curl http://localhost:8081/subjects

# Voir les logs
docker-compose logs schema-registry

# V√©rifier la g√©n√©ration des classes
ls -la src/main/java/com/example/avro/
```

#### 7. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 07 : Deployer un kafka connect - 60min

### üéØ Objectifs
- D√©ployer et configurer Kafka Connect
- Installer des connecteurs JDBC et MongoDB
- Cr√©er un pipeline de donn√©es PostgreSQL ‚Üí Kafka ‚Üí MongoDB
- Comprendre les connecteurs Source et Sink

### üìã Pr√©requis
- Docker Compose avec suffisamment de ressources
- Compr√©hension des concepts ETL
- Notions de PostgreSQL et MongoDB

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement complet
```bash
# D√©marrer le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center postgres connect mongodb
```

#### 2. Installer un connecteur JDBC
```bash
docker-compose exec -u root connect confluent-hub install confluentinc/kafka-connect-jdbc:10.7.6
```

**Output attendu :**
```console
The component can be installed in any of the following Confluent Platform installations: 
1. / (installed rpm/deb package)
2. / (where this tool is installed)
   Choose one of these to continue the installation (1-2): 1
   Do you want to install this into /usr/share/confluent-hub-components? (yN) N

Specify installation directory: /usr/share/java/kafka

Component's license:
Confluent Community License
https://www.confluent.io/confluent-community-license
I agree to the software license agreement (yN) y

Downloading component Kafka Connect JDBC 10.7.6, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/java/kafka
Detected Worker's configs:
1. Standard: /etc/kafka/connect-distributed.properties
2. Standard: /etc/kafka/connect-standalone.properties
3. Standard: /etc/schema-registry/connect-avro-distributed.properties
4. Standard: /etc/schema-registry/connect-avro-standalone.properties
5. Used by Connect process with PID : /etc/kafka-connect/kafka-connect.properties
   Do you want to update all detected configs? (yN) y

Adding installation directory to plugin path in the following files:
/etc/kafka/connect-distributed.properties
/etc/kafka/connect-standalone.properties
/etc/schema-registry/connect-avro-distributed.properties
/etc/schema-registry/connect-avro-standalone.properties
/etc/kafka-connect/kafka-connect.properties

Completed
```

#### 3. Installer un connecteur MongoDB
```bash
docker-compose exec -u root connect confluent-hub install mongodb/kafka-connect-mongodb:latest
```

**Output attendu :**
```console
The component can be installed in any of the following Confluent Platform installations: 
1. / (installed rpm/deb package)
2. / (where this tool is installed)
   Choose one of these to continue the installation (1-2): 1
   Do you want to install this into /usr/share/confluent-hub-components? (yN) N

Specify installation directory: /usr/share/java/kafka

Component's license:
Confluent Community License
https://www.confluent.io/confluent-community-license
I agree to the software license agreement (yN) y

Downloading component Kafka Connect JDBC 10.7.6, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/java/kafka
Detected Worker's configs:
1. Standard: /etc/kafka/connect-distributed.properties
2. Standard: /etc/kafka/connect-standalone.properties
3. Standard: /etc/schema-registry/connect-avro-distributed.properties
4. Standard: /etc/schema-registry/connect-avro-standalone.properties
5. Used by Connect process with PID : /etc/kafka-connect/kafka-connect.properties
   Do you want to update all detected configs? (yN) y

Adding installation directory to plugin path in the following files:
/etc/kafka/connect-distributed.properties
/etc/kafka/connect-standalone.properties
/etc/schema-registry/connect-avro-distributed.properties
/etc/schema-registry/connect-avro-standalone.properties
/etc/kafka-connect/kafka-connect.properties

Completed
```

#### 4. Red√©marrer le connecteur
```bash
docker-compose restart connect
```

#### 5. Pr√©parer la base PostgreSQL
```bash
# √Ä l'int√©rieur du conteneur PostgreSQL, cr√©er une table `users`
docker-compose exec postgres psql -U myuser -d lab
```

```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### 6. Configurer le connecteur JDBC Source
**Cr√©er le fichier `jdbc-source-config.json`** pour le connecteur JDBC Source afin d'extraire les donn√©es de cette table users et de les publier dans un topic Kafka :

```json
{
  "name":"jdbc-source-connector",
  "config":{
    "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max":"1",
    "connection.url":"jdbc:postgresql://postgres:5432/lab",
    "connection.user":"myuser",
    "connection.password":"mypassword",
    "table.whitelist":"users",
    "mode":"incrementing",
    "incrementing.column.name":"id",
    "poll.interval.ms":"10000",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}
```

#### 7. D√©ployer le connecteur Source JDBC
```bash
curl -X POST -H "Content-Type: application/json" --data @jdbc-source-config.json http://connect:8083/connectors
```

#### 8. V√©rifier l'√©tat du connecteur JDBC Source
```bash
curl -X GET http://connect:8083/connectors/jdbc-source-connector/status
```

#### 9. Configurer le connecteur MongoDB Sink
**Cr√©er le fichier `mongo-sink-config.json`** pour le connecteur MongoDB Sink afin d'extraire les donn√©es de ce topic users et de les stocker dans MongoDB :

```json
{
  "name": "mongodb-sink-connector",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "tasks.max": "1",
    "topics": "users",
    "connection.uri": "mongodb://myuser:mypassword@mongodb:27017",
    "database": "lab",
    "collection": "users",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false"
  }
}
```

#### 10. D√©ployer le connecteur MongoDB Sink
```bash
curl -X POST -H "Content-Type: application/json" --data @mongo-sink-config.json http://connect:8083/connectors
```

#### 11. V√©rifier l'√©tat du connecteur Mongo Sink
```bash
curl -X GET http://connect:8083/connectors/mongodb-sink-connector/status
```

#### 12. Tester le pipeline avec des donn√©es
```bash
# Acc√©der au conteneur PostgreSQL et ins√©rer des donn√©es d'exemple dans la table `users`
docker-compose exec postgres psql -U myuser -d lab
```

```sql
INSERT INTO users (name, email) VALUES
('toto', 'toto@example.com'),
('titi', 'titi@example.com'),
('tata', 'tata@example.com');
```

#### 13. V√©rifier les donn√©es dans MongoDB
```bash
# √Ä l'int√©rieur du conteneur
docker-compose exec mongodb /bin/bash
```

```bash
# Se connecter au serveur
mongosh "mongodb://myuser:mypassword@mongodb"
```

```bash
# Utiliser la base de donn√©es `lab`
use lab;
```

```bash
# Faire une requ√™te `find`
db.users.find();
```

### ‚úÖ Validation
- [ ] Tous les conteneurs d√©marr√©s correctement
- [ ] Connecteurs JDBC et MongoDB install√©s et configur√©s
- [ ] Pipeline PostgreSQL ‚Üí Kafka ‚Üí MongoDB fonctionnel
- [ ] Donn√©es synchronis√©es entre PostgreSQL et MongoDB
- [ ] Connecteurs visibles dans Control Center

### üîß En cas de probl√®me
```bash
# V√©rifier l'√©tat des connecteurs
curl http://localhost:8083/connectors
curl http://localhost:8083/connectors/jdbc-source-connector/status
curl http://localhost:8083/connectors/mongodb-sink-connector/status

# Voir les logs de Connect
docker-compose logs connect

# Red√©marrer Connect
docker-compose restart connect

# Supprimer un connecteur si besoin
curl -X DELETE http://localhost:8083/connectors/jdbc-source-connector
```

#### 14. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 08 : Basic kafka stream - 60min

### üéØ Objectifs
- Cr√©er une application Kafka Streams simple qui lit des donn√©es depuis un topic source
- Traiter les donn√©es en temps r√©el
- √âcrire les r√©sultats dans un topic de destination
- Comprendre les concepts de transformation de flux

### üìã Pr√©requis
- Java et Maven install√©s
- Cluster Kafka fonctionnel
- Notions de programmation r√©active et streaming

### üõ†Ô∏è Instructions

#### 1. Cloner le projet (si pas d√©j√† fait)
```bash
# Ex√©cuter la commande suivante dans le terminal pour cloner le projet (cette √©tape n'est pas n√©cessaire si vous l'avez d√©j√† faite dans le laboratoire pr√©c√©dent)
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module kafkastream
```bash
cd labs_kafka/kafkastream
```

#### 3. Compl√©ter le code
Completez le code pour `StreamsConfig`, `StreamsBuilder` et `KafkaStreams`.

#### 4. D√©marrer le cluster Kafka
```bash
# D√©marrer le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter kafkastream
Construire et ex√©cuter l'application Kafka Streams selon vos pr√©f√©rences (IDE ou ligne de commande).

### ‚úÖ Validation
- [ ] Application Kafka Streams compile et d√©marre
- [ ] Topics source et destination cr√©√©s
- [ ] Transformation des donn√©es fonctionnelle
- [ ] Flux de donn√©es visible dans Control Center
- [ ] Pas d'erreur dans les logs de l'application

### üîß En cas de probl√®me
```bash
# V√©rifier les topics
docker-compose exec broker kafka-topics --list --bootstrap-server broker:9092

# Voir les consumer groups
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --list

# Reset l'application si besoin
docker-compose exec broker kafka-streams-application-reset --application-id [app-id] --bootstrap-servers broker:9092
```

#### 6. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

## Lab 09 : KsqlDB - 60min

### üéØ Objectifs
- Utiliser ksqlDB pour effectuer des requ√™tes SQL sur les flux Kafka
- Cr√©er des streams et tables avec SQL
- Effectuer des agr√©gations en temps r√©el
- Comprendre les concepts de streaming SQL

### üìã Pr√©requis
- Connaissances SQL de base
- Cluster Kafka avec ksqlDB
- Notion de streaming vs tables

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec ksqlDB
```bash
# D√©marrer le cluster Kafka avec l'option -d pour l'ex√©cuter en mode d√©tach√©
docker-compose up -d broker control-center ksqldb-server ksqldb-cli
```

#### 2. Produire des donn√©es de test
```bash
# Utiliser la console Kafka pour produire des messages vers le topic `users`

# Ex√©cuter dans la console du conteneur `broker`
docker-compose exec broker /bin/bash
```

```bash
# Ex√©cuter le producteur
kafka-console-producer --bootstrap-server kafka:9092 --topic users
```

#### 3. Saisir les messages JSON
Saisir les messages JSON suivants, chacun repr√©sentant un enregistrement `user` :

```json
{"id": 1, "name": "toto", "email": "toto.doe@example.com", "created_at": "2024-06-23T12:00:00Z"}
```

```json
{"id": 2, "name": "titi", "email": "titi@example.com", "created_at": "2024-05-23T12:05:00Z"}
```

```json
{"id": 3, "name": "tata", "email": "tata@example.com", "created_at": "2024-06-23T12:05:00Z"}
```

```json
{"id": 4, "name": "jo", "email": "jo.doe@example.com", "created_at": "2024-05-23T12:00:00Z"}
```

```json
{"id": 5, "name": "ohe", "email": "ohe@example.com", "created_at": "2024-05-23T12:05:00Z"}
```

```json
{"id": 6, "name": "yao", "email": "yao@example.com", "created_at": "2024-06-23T12:05:00Z"}
```

#### 4. Acc√©der √† l'interface ksqlDB
Quitter le conteneur `broker` et acc√©der √† l'interface web du serveur ksqlDB en naviguant vers `http://localhost:8088` dans votre navigateur. Cela devrait afficher l'interface du serveur ksqlDB.

#### 5. Acc√©der √† l'interface CLI de ksqlDB
```bash
# Acc√©der √† l'interface en ligne de commande (CLI) de ksqlDB pour ex√©cuter des requ√™tes ksqlDB
docker-compose exec ksqldb-cli bash
```

#### 6. Se connecter au serveur ksqlDB
```bash
# √Ä l'int√©rieur du shell, connecter au serveur ksqlDB en utilisant l'interface en ligne de commande (CLI) de ksqlDB

# Connecter au serveur ksqlDB
ksql http://ksqldb-server:8088
```

```bash
# D√©finir l'offset sur Earliest
SET 'auto.offset.reset' = 'earliest';
```

#### 7. Cr√©er un stream ksqlDB
```sql
-- Cr√©er un stream dans ksqlDB en utilisant la requ√™te suivante. Par exemple, pour cr√©er un stream pour le topic `users`
CREATE STREAM users_stream (id INT, name VARCHAR, email VARCHAR, created_at VARCHAR) WITH (KAFKA_TOPIC='users',VALUE_FORMAT='JSON');
```

#### 8. Interroger le stream
```sql
-- Interroger le `users_stream` pour voir les donn√©es
SELECT * FROM users_stream EMIT CHANGES;
```

#### 9. Cr√©er une table d'agr√©gation
```sql
-- Cr√©er une table appel√©e `user_counts` qui stocke le nombre `users` group√©s par leur `created_at`
CREATE TABLE user_counts AS
SELECT created_at, COUNT(*) AS count
FROM users_stream
GROUP BY created_at;
```

#### 10. Interroger la table
```sql
-- Interroger `user_counts` pour voir les donn√©es
SELECT * FROM user_counts EMIT CHANGES;
```

### ‚úÖ Validation
- [ ] ksqlDB Server accessible sur le port 8088
- [ ] Interface web ksqlDB fonctionnelle
- [ ] Stream `users_stream` cr√©√© avec succ√®s
- [ ] Table `user_counts` montre les agr√©gations correctes
- [ ] Requ√™tes en temps r√©el fonctionnelles

### üîß En cas de probl√®me
```sql
-- Voir les streams et tables existants
SHOW STREAMS;
SHOW TABLES;

-- D√©crire un stream
DESCRIBE users_stream;

-- Supprimer un stream si besoin
DROP STREAM IF EXISTS users_stream;
```

```bash
# Voir les logs ksqlDB
docker-compose logs ksqldb-server

# Red√©marrer ksqlDB
docker-compose restart ksqldb-server ksqldb-cli
```

#### 11. Arr√™ter le cluster
```bash
# Arr√™ter le cluster Kafka avec l'option -v pour supprimer les volumes
docker-compose down -v
```

---

Vous pouvez m'envoyer un e-mail √† [mohamedkaraga@yahoo.fr](mailto:mohamedkaraga@yahoo.fr).
