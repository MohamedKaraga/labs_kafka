# Labs Kafka - Formation Developer

## Lab 01 : Installation Sans Docker - 60min

### üéØ Objectifs
- Configurer Kafka en mode KRaft
- Cr√©er et tester des topics
- Produire et consommer des messages

### üìã Pr√©requis
- **JDK 11+ install√©** (JDK 17+ recommand√© pour Kafka 4.0)
- Droits administrateur

**üí° Choix de version :**
- **Kafka 3.9** : Compatible avec Java 11+ (recommand√© pour la compatibilit√©)
- **Kafka 4.0** : N√©cessite obligatoirement Java 17+

### üõ†Ô∏è Instructions

#### 1. T√©l√©charger et installer Kafka

**Option A : Kafka 3.9 (compatible Java 11+) - RECOMMAND√â**

```bash
wget https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz
tar -xzf kafka_2.13-3.9.0.tgz
cd kafka_2.13-3.9.0
```

**Option B : Kafka 4.0 (n√©cessite Java 17+)**

```bash
wget https://downloads.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
tar -xzf kafka_2.13-4.0.0.tgz
cd kafka_2.13-4.0.0
```

**V√©rifier Java**

```bash
java -version
```

#### 2. Cr√©er les r√©pertoires et configurer

```bash
sudo mkdir -p /var/lib/kafka/data
sudo mkdir -p /var/lib/kafka/meta
sudo chown -R $(whoami):$(whoami) /var/lib/kafka
```

**Cr√©er la configuration**

```bash
cat > config/server.properties << EOF
# Configuration KRaft
process.roles=broker,controller
node.id=1
controller.quorum.voters=1@localhost:9093

# Listeners
listeners=PLAINTEXT://localhost:9092,CONTROLLER://localhost:9093
controller.listener.names=CONTROLLER
listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT

# R√©pertoires
log.dirs=/var/lib/kafka/data
metadata.log.dir=/var/lib/kafka/meta

# Options
auto.create.topics.enable=true
EOF
```

#### 3. Formater et d√©marrer Kafka

```bash
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/server.properties
```

```bash
bin/kafka-server-start.sh config/server.properties
```

#### 4. Tester dans de nouveaux terminaux

**Terminal 2 : Cr√©er un topic**

```bash
bin/kafka-topics.sh --create --topic test --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092
```

**D√©crire le topic**

```bash
bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092
```

**Terminal 3 : Producer**

```bash
bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092
```

**Exemples de messages √† saisir :**

```
Formation Kafka avec Orsys
Hello World Kafka
{"event": "user_login", "user_id": 123, "timestamp": "2024-12-16T10:30:00Z"}
{"sensor_id": "TEMP_001", "temperature": 22.5, "location": "Paris"}
```

**Terminal 4 : Consumer**

```bash
bin/kafka-console-consumer.sh --topic test --from-beginning --bootstrap-server localhost:9092
```

### ‚úÖ Validation
- [ ] Kafka d√©marre sans erreur dans le terminal 1
- [ ] Topic `test` cr√©√© avec succ√®s
- [ ] Messages tap√©s dans le producer apparaissent dans le consumer

### üîß En cas de probl√®me

**V√©rifier Java**

```bash
java -version
```

**Installer Java 11 (Ubuntu/Debian)**

```bash
sudo apt update && sudo apt install openjdk-11-jdk
```

**Installer Java 11 (CentOS/RHEL)**

```bash
sudo yum install java-11-openjdk-devel
```

**Installer Java 11 (macOS)**

```bash
brew install openjdk@11
```

**Installer Java 17 (Ubuntu/Debian)**

```bash
sudo apt update && sudo apt install openjdk-17-jdk
```

**Installer Java 17 (CentOS/RHEL)**

```bash
sudo yum install java-17-openjdk-devel
```

**Installer Java 17 (macOS)**

```bash
brew install openjdk@17
```

**D√©finir JAVA_HOME**

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk
```

**macOS**

```bash
export JAVA_HOME=/opt/homebrew/opt/openjdk@11
```

**Corriger les permissions**

```bash
sudo chown -R $(whoami):$(whoami) /var/lib/kafka
```

**Tuer les processus Kafka**

```bash
ps aux | grep kafka | awk '{print $2}' | xargs kill -9
```

**Nettoyer et reformater (erreur "Invalid cluster.id")**

```bash
ps aux | grep kafka | awk '{print $2}' | xargs kill -9
sudo rm -rf /var/lib/kafka/data/*
sudo rm -rf /var/lib/kafka/meta/*
bin/kafka-storage.sh format -t $(bin/kafka-storage.sh random-uuid) -c config/server.properties
bin/kafka-server-start.sh config/server.properties
```

---

## Lab 02 : Kafka avec Docker - 45min

### üéØ Objectifs
- D√©ployer Kafka avec Docker
- Utiliser Control Center pour le monitoring
- Comprendre l'architecture conteneuris√©e

### üìã Pr√©requis
- Docker et Docker Compose install√©s
- 8GB RAM disponible

### üõ†Ô∏è Instructions

#### 1. R√©cup√©rer le fichier Docker Compose

```bash
wget https://raw.githubusercontent.com/MohamedKaraga/labs_kafka/refs/heads/master/docker-compose.yml
```

**V√©rifier Docker**

```bash
docker ps
```

#### 2. D√©marrer les services

```bash
docker-compose up -d broker control-center
```

**V√©rifier le d√©marrage (attendre ~30 secondes)**

```bash
docker-compose ps
```

#### 3. Tester Kafka

**Entrer dans le conteneur**

```bash
docker-compose exec broker /bin/bash
```

**Cr√©er un topic**

```bash
kafka-topics --bootstrap-server broker:9092 --create --topic test --partitions 1 --replication-factor 1
```

**Lister les topics**

```bash
kafka-topics --bootstrap-server broker:9092 --list
```

#### 4. Test Producer/Consumer

**Terminal 1 : Producer (dans le conteneur)**

```bash
kafka-console-producer --bootstrap-server broker:9092 --topic test
```

**Terminal 2 : Consumer (nouveau terminal)**

```bash
docker-compose exec broker /bin/bash
kafka-console-consumer --bootstrap-server broker:9092 --from-beginning --topic test
```

### ‚úÖ Validation
- [ ] Conteneurs `broker` et `control-center` en statut "Up"
- [ ] Control Center accessible sur http://localhost:9021
- [ ] Messages √©chang√©s entre producer et consumer

### üîß En cas de probl√®me

**Red√©marrer proprement**

```bash
docker-compose down -v
docker-compose up -d broker control-center
```

**Voir les logs**

```bash
docker-compose logs broker
```

#### 5. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 03 : Producer - 60min

### üéØ Objectifs
- Simuler des producteurs Kafka int√©gr√©s dans des appareils IoT
- Configurer le producteur pour publier les donn√©es des capteurs
- Configurer le batching et la compression

### üìã Pr√©requis
- Java et Maven install√©s
- Cluster Kafka fonctionnel (Lab 02)
- Notions de programmation Java

### üõ†Ô∏è Instructions

#### 1. Cloner le projet

```bash
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module producteur

```bash
cd labs_kafka/producer
```

#### 3. Compl√©ter le code

Completez le code pour `ProducerConfig`, `ProducerRecord`, et `KafkaProducer`.

#### 4. D√©marrer le cluster Kafka

```bash
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter le producteur

Construire et ex√©cuter le producteur selon vos pr√©f√©rences (IDE ou ligne de commande).

#### 6. Tester les configurations de performance

Ex√©cutez le producteur avec diff√©rentes configurations pour `linger.ms` et `batch.size`.

#### 7. Configurer la compression

Configurez le type de compression (`snappy`, `gzip`, ou `lz4`).

### ‚úÖ Validation
- [ ] Code compile et s'ex√©cute sans erreur
- [ ] Messages des capteurs visibles dans Control Center
- [ ] Diff√©rences de performance observ√©es
- [ ] Impact de la compression analys√©

### üîß En cas de probl√®me

**V√©rifier les conteneurs**

```bash
docker-compose ps
```

**Voir les logs**

```bash
docker-compose logs broker
```

**Red√©marrer**

```bash
docker-compose down -v
docker-compose up -d broker control-center
```

#### 8. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 04 : Consumer - 60min

### üéØ Objectifs
- Configurer le consommateur Kafka
- Traiter les donn√©es entrantes
- Ajuster les param√®tres de performance
- Impl√©menter le commit manuel

### üìã Pr√©requis
- Producer du Lab 03 fonctionnel
- Java et Maven install√©s
- Compr√©hension des consumer groups

### üõ†Ô∏è Instructions

#### 1. Cloner le projet (si pas d√©j√† fait)

```bash
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module consommateur

```bash
cd labs_kafka/consumer
```

#### 3. Compl√©ter le code

Completez le code pour `ConsumerConfig` et `KafkaConsumer`.

#### 4. D√©marrer le cluster Kafka

```bash
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter le consommateur

Construire et ex√©cuter le consommateur.

#### 6. Alimenter le topic

Relancez le producer du Lab 03.

#### 7. Tester les configurations

Testez avec `fetch.min.bytes (5000000)` et `fetch.max.wait.ms (5000)`.

#### 8. Conversion vers commit manuel

Impl√©mentez le commit manuel pour garantir le traitement "exactly-once".

### ‚úÖ Validation
- [ ] Consumer re√ßoit et traite les messages
- [ ] Configurations de performance test√©es
- [ ] Commit manuel impl√©ment√©
- [ ] Robustesse test√©e
- [ ] M√©triques visibles dans Control Center

### üîß En cas de probl√®me

**V√©rifier les consumer groups**

```bash
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --list
```

**Voir les d√©tails du groupe**

```bash
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --describe --group [group-name]
```

**Reset des offsets**

```bash
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --group [group-name] --reset-offsets --to-earliest --topic [topic-name] --execute
```

#### 9. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 05 : Kafka REST Proxy - 45min

### üéØ Objectifs
- Utiliser l'API REST de Kafka
- Comprendre l'int√©gration HTTP avec Kafka
- Manipuler des donn√©es JSON via REST

### üìã Pr√©requis
- Docker et Docker Compose install√©s
- Notions de base des API REST
- Compr√©hension de curl

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec REST Proxy

```bash
docker-compose up -d broker control-center rest-proxy
```

#### 2. Acc√©der au conteneur broker

```bash
docker-compose exec broker /bin/bash
```

**Cr√©er un topic**

```bash
kafka-topics --create --topic bar --bootstrap-server broker:9092 --partitions 1 --replication-factor 1
```

**Produire un message JSON**

```bash
curl -X POST -H "Content-Type: application/vnd.kafka.json.v2+json" \
--data '{"records":[{"value":{"name":"toto", "age":30}}]}' \
http://localhost:8082/topics/bar
```

#### 3. Consommer des messages

**Cr√©er une instance consumer**

```bash
curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
--data '{"name": "my_consumer_instance", "format": "json", "auto.offset.reset": "earliest"}' \
http://localhost:8082/consumers/my_consumer_group
```

**Abonner le consommateur**

```bash
curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
--data '{"topics":["bar"]}' \
http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/subscription
```

**Consommer des messages**

```bash
curl -X GET -H "Accept: application/vnd.kafka.json.v2+json" \
http://localhost:8082/consumers/my_consumer_group/instances/my_consumer_instance/records
```

### ‚úÖ Validation
- [ ] REST Proxy accessible sur le port 8082
- [ ] Messages JSON produits avec succ√®s
- [ ] Messages consomm√©s via l'API REST
- [ ] Consumer instance cr√©√©e correctement

### üîß En cas de probl√®me

**V√©rifier REST Proxy**

```bash
docker-compose logs rest-proxy
```

**Tester la connectivit√©**

```bash
curl http://localhost:8082/topics
```

**Red√©marrer**

```bash
docker-compose down -v
docker-compose up -d broker control-center rest-proxy
```

#### 4. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 06 : Schema Registry - 60min

### üéØ Objectifs
- Comprendre la gestion des sch√©mas
- Utiliser Apache Avro pour la s√©rialisation
- Transformer les producers/consumers
- G√©n√©rer des classes Java depuis sch√©mas Avro

### üìã Pr√©requis
- Java et Maven install√©s
- Producer et Consumer des labs pr√©c√©dents
- Connaissance de la s√©rialisation

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec Schema Registry

```bash
docker-compose up -d broker control-center schema-registry
```

#### 2. Enregistrer le sch√©ma

```bash
curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \
--data '{"schema": "{\"type\":\"record\",\"name\":\"User\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"age\",\"type\":\"int\"},{\"name\":\"email\",\"type\":[\"null\",\"string\"],\"default\":null}]}"}' \
http://localhost:8081/subjects/user-value/versions
```

#### 3. Configuration Maven

**Ajouter le repository Maven**

```xml
<repositories>
  <repository>
    <id>confluent</id>
    <url>https://packages.confluent.io/maven/</url>
  </repository>
</repositories>
```

**Ajouter les d√©pendances**

```xml
<!-- Avro dependencies -->
<dependency>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro</artifactId>
    <version>1.11.1</version>
</dependency>
<!-- Schema Registry dependencies -->
<dependency>
   <groupId>io.confluent</groupId>
   <artifactId>kafka-avro-serializer</artifactId>
   <version>7.6.0</version>
</dependency>
```

#### 4. D√©finir le sch√©ma Avro

Cr√©er le r√©pertoire `src/main/avro` et le fichier `user.avsc` :

```json
{
  "type": "record",
  "name": "User",
  "namespace": "com.example.avro",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"}
  ]
}
```

**Ajouter le plugin Maven**

```xml
<build>
  <plugins>
      <plugin>
          <groupId>org.apache.avro</groupId>
          <artifactId>avro-maven-plugin</artifactId>
          <version>1.11.1</version>
          <executions>
              <execution>
                  <phase>generate-sources</phase>
                  <goals>
                      <goal>schema</goal>
                  </goals>
                  <configuration>
                      <sourceDirectory>${project.basedir}/src/main/avro</sourceDirectory>
                      <outputDirectory>${project.basedir}/src/main/java</outputDirectory>
                  </configuration>
              </execution>
          </executions>
      </plugin>
  </plugins>
</build>
```

#### 5. G√©n√©rer les classes Java

```bash
mvn clean compile
```

#### 6. Configuration Producer avec Avro

```java
Properties props = new Properties();
props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class.getName());
props.put("schema.registry.url", "http://localhost:8081");

KafkaProducer<String, User> producer = new KafkaProducer<>(props);

User user = new User("John Doe", 30);

ProducerRecord<String, User> record = new ProducerRecord<>("users", user.getName().toString(), user);
```

#### 7. Configuration Consumer avec Avro

```java
Properties props = new Properties();
props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(ConsumerConfig.GROUP_ID_CONFIG, "user-consumer-group");
props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
props.put("schema.registry.url", "http://localhost:8081");
props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

KafkaConsumer<String, User> consumer = new KafkaConsumer<>(props);
```

### ‚úÖ Validation
- [ ] Schema Registry accessible sur le port 8081
- [ ] Sch√©ma User enregistr√©
- [ ] Classes Java g√©n√©r√©es
- [ ] Producer envoie des messages Avro
- [ ] Consumer d√©s√©rialise correctement

### üîß En cas de probl√®me

**V√©rifier Schema Registry**

```bash
curl http://localhost:8081/subjects
```

**Voir les logs**

```bash
docker-compose logs schema-registry
```

**V√©rifier la g√©n√©ration**

```bash
ls -la src/main/java/com/example/avro/
```

#### 8. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 07 : Deployer un kafka connect - 60min

### üéØ Objectifs
- D√©ployer et configurer Kafka Connect
- Installer des connecteurs JDBC et MongoDB
- Cr√©er un pipeline PostgreSQL ‚Üí Kafka ‚Üí MongoDB
- Comprendre les connecteurs Source et Sink

### üìã Pr√©requis
- Docker Compose avec suffisamment de ressources
- Compr√©hension des concepts ETL
- Notions de PostgreSQL et MongoDB

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement complet

```bash
docker-compose up -d broker control-center postgres connect mongodb
```

#### 2. Installer le connecteur JDBC

```bash
docker-compose exec -u root connect confluent-hub install confluentinc/kafka-connect-jdbc:10.7.6
```

R√©pondre aux invites comme suit :
- Choose installation: `1`
- Install into /usr/share/confluent-hub-components? `N`
- Specify directory: `/usr/share/java/kafka`
- Agree to license: `y`
- Update all configs: `y`

#### 3. Installer le connecteur MongoDB

```bash
docker-compose exec -u root connect confluent-hub install mongodb/kafka-connect-mongodb:latest
```

Suivre les m√™mes r√©ponses que pour JDBC.

#### 4. Red√©marrer Connect

```bash
docker-compose restart connect
```

#### 5. Pr√©parer PostgreSQL

```bash
docker-compose exec postgres psql -U myuser -d lab
```

**Cr√©er la table**

```sql
CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### 6. Configuration JDBC Source

Cr√©er `jdbc-source-config.json` :

```json
{
  "name":"jdbc-source-connector",
  "config":{
    "connector.class":"io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max":"1",
    "connection.url":"jdbc:postgresql://postgres:5432/lab",
    "connection.user":"myuser",
    "connection.password":"mypassword",
    "table.whitelist":"users",
    "mode":"incrementing",
    "incrementing.column.name":"id",
    "poll.interval.ms":"10000",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}
```

#### 7. D√©ployer JDBC Source

```bash
curl -X POST -H "Content-Type: application/json" --data @jdbc-source-config.json http://connect:8083/connectors
```

#### 8. V√©rifier JDBC Source

```bash
curl -X GET http://connect:8083/connectors/jdbc-source-connector/status
```

#### 9. Configuration MongoDB Sink

Cr√©er `mongo-sink-config.json` :

```json
{
  "name": "mongodb-sink-connector",
  "config": {
    "connector.class": "com.mongodb.kafka.connect.MongoSinkConnector",
    "tasks.max": "1",
    "topics": "users",
    "connection.uri": "mongodb://myuser:mypassword@mongodb:27017",
    "database": "lab",
    "collection": "users",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false"
  }
}
```

#### 10. D√©ployer MongoDB Sink

```bash
curl -X POST -H "Content-Type: application/json" --data @mongo-sink-config.json http://connect:8083/connectors
```

#### 11. V√©rifier MongoDB Sink

```bash
curl -X GET http://connect:8083/connectors/mongodb-sink-connector/status
```

#### 12. Ins√©rer des donn√©es de test

```bash
docker-compose exec postgres psql -U myuser -d lab
```

```sql
INSERT INTO users (name, email) VALUES
('toto', 'toto@example.com'),
('titi', 'titi@example.com'),
('tata', 'tata@example.com');
```

#### 13. V√©rifier dans MongoDB

```bash
docker-compose exec mongodb /bin/bash
```

```bash
mongosh "mongodb://myuser:mypassword@mongodb"
```

```javascript
use lab;
db.users.find();
```

### ‚úÖ Validation
- [ ] Tous les conteneurs d√©marr√©s
- [ ] Connecteurs install√©s et configur√©s
- [ ] Pipeline fonctionnel
- [ ] Donn√©es synchronis√©es
- [ ] Connecteurs visibles dans Control Center

### üîß En cas de probl√®me

**V√©rifier les connecteurs**

```bash
curl http://localhost:8083/connectors
curl http://localhost:8083/connectors/jdbc-source-connector/status
curl http://localhost:8083/connectors/mongodb-sink-connector/status
```

**Voir les logs**

```bash
docker-compose logs connect
```

**Red√©marrer Connect**

```bash
docker-compose restart connect
```

**Supprimer un connecteur**

```bash
curl -X DELETE http://localhost:8083/connectors/jdbc-source-connector
```

#### 14. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 08 : Basic kafka stream - 60min

### üéØ Objectifs
- Cr√©er une application Kafka Streams
- Traiter les donn√©es en temps r√©el
- √âcrire les r√©sultats dans un topic
- Comprendre les transformations de flux

### üìã Pr√©requis
- Java et Maven install√©s
- Cluster Kafka fonctionnel
- Notions de programmation r√©active

### üõ†Ô∏è Instructions

#### 1. Cloner le projet (si pas d√©j√† fait)

```bash
git clone https://github.com/MohamedKaraga/labs_kafka.git
```

#### 2. Aller dans le module kafkastream

```bash
cd labs_kafka/kafkastream
```

#### 3. Compl√©ter le code

Completez le code pour `StreamsConfig`, `StreamsBuilder` et `KafkaStreams`.

#### 4. D√©marrer le cluster Kafka

```bash
docker-compose up -d broker control-center
```

#### 5. Construire et ex√©cuter

Construire et ex√©cuter l'application selon vos pr√©f√©rences (IDE ou ligne de commande).

### ‚úÖ Validation
- [ ] Application compile et d√©marre
- [ ] Topics cr√©√©s
- [ ] Transformation fonctionnelle
- [ ] Flux visible dans Control Center
- [ ] Pas d'erreur dans les logs

### üîß En cas de probl√®me

**V√©rifier les topics**

```bash
docker-compose exec broker kafka-topics --list --bootstrap-server broker:9092
```

**Voir les consumer groups**

```bash
docker-compose exec broker kafka-consumer-groups --bootstrap-server broker:9092 --list
```

**Reset l'application**

```bash
docker-compose exec broker kafka-streams-application-reset --application-id [app-id] --bootstrap-servers broker:9092
```

#### 6. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Lab 09 : KsqlDB - 60min

### üéØ Objectifs
- Utiliser ksqlDB pour des requ√™tes SQL sur Kafka
- Cr√©er des streams et tables avec SQL
- Effectuer des agr√©gations en temps r√©el
- Comprendre le streaming SQL

### üìã Pr√©requis
- Connaissances SQL de base
- Cluster Kafka avec ksqlDB
- Notion de streaming vs tables

### üõ†Ô∏è Instructions

#### 1. D√©marrer l'environnement avec ksqlDB

```bash
docker-compose up -d broker control-center ksqldb-server ksqldb-cli
```

#### 2. Produire des donn√©es de test

```bash
docker-compose exec broker /bin/bash
```

```bash
kafka-console-producer --bootstrap-server kafka:9092 --topic users
```

**Saisir les messages JSON :**

```json
{"id": 1, "name": "toto", "email": "toto.doe@example.com", "created_at": "2024-06-23T12:00:00Z"}
{"id": 2, "name": "titi", "email": "titi@example.com", "created_at": "2024-05-23T12:05:00Z"}
{"id": 3, "name": "tata", "email": "tata@example.com", "created_at": "2024-06-23T12:05:00Z"}
{"id": 4, "name": "jo", "email": "jo.doe@example.com", "created_at": "2024-05-23T12:00:00Z"}
{"id": 5, "name": "ohe", "email": "ohe@example.com", "created_at": "2024-05-23T12:05:00Z"}
{"id": 6, "name": "yao", "email": "yao@example.com", "created_at": "2024-06-23T12:05:00Z"}
```

#### 3. Acc√©der √† ksqlDB CLI

```bash
docker-compose exec ksqldb-cli bash
```

```bash
ksql http://ksqldb-server:8088
```

**Configurer l'offset**

```sql
SET 'auto.offset.reset' = 'earliest';
```

#### 4. Cr√©er un stream

```sql
CREATE STREAM users_stream (id INT, name VARCHAR, email VARCHAR, created_at VARCHAR) 
WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON');
```

#### 5. Interroger le stream

```sql
SELECT * FROM users_stream EMIT CHANGES;
```

#### 6. Cr√©er une table d'agr√©gation

```sql
CREATE TABLE user_counts AS
SELECT created_at, COUNT(*) AS count
FROM users_stream
GROUP BY created_at;
```

#### 7. Interroger la table

```sql
SELECT * FROM user_counts EMIT CHANGES;
```

### ‚úÖ Validation
- [ ] ksqlDB Server accessible sur le port 8088
- [ ] Interface web ksqlDB fonctionnelle
- [ ] Stream `users_stream` cr√©√© avec succ√®s
- [ ] Table `user_counts` montre les agr√©gations correctes
- [ ] Requ√™tes en temps r√©el fonctionnelles

### üîß En cas de probl√®me

**Voir les streams et tables**

```sql
SHOW STREAMS;
SHOW TABLES;
```

**D√©crire un stream**

```sql
DESCRIBE users_stream;
```

**Supprimer un stream**

```sql
DROP STREAM IF EXISTS users_stream;
```

**Voir les logs ksqlDB**

```bash
docker-compose logs ksqldb-server
```

**Red√©marrer ksqlDB**

```bash
docker-compose restart ksqldb-server ksqldb-cli
```

#### 8. Arr√™ter le cluster

```bash
docker-compose down -v
```

---

## Contact

Pour toute question, vous pouvez me contacter √† [mohamedkaraga@yahoo.fr](mailto:mohamedkaraga@yahoo.fr).
